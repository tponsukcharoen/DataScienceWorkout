{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CZI coding test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: only words and terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda2\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "#Install nltk if needed\n",
    "#!pip install nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#Set up tokenizeer\n",
    "import nltk #this is for tokenizing files\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #This ensure no symbols in tokens.\n",
    "\n",
    "#To make dictionary \"able to sort\"\n",
    "import operator\n",
    "\n",
    "#To fix encodiing issue\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up the directory\n",
    "DataDir = r'C:/Users/Admin/Downloads/data/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = os.listdir(DataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the task below, we can build a dictionary where the key is the word and the value is the count of the word. Similarly, we can build a dictionary of bi-gram (Word1 Word2) as well. In Task 1, we need a dictionary for an individual file. In Task 2,  we need a single dictionary for the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_dicts = []\n",
    "\n",
    "for i in range(0,len(filenames)):\n",
    "    file_content = open(DataDir+filenames[i]).read()\n",
    "    tokens = tokenizer.tokenize(file_content)\n",
    "    #initialize the unigram\n",
    "    unigram_dicts.append({})\n",
    "    #populate the unigram. We don't need to count the number but just for fun.\n",
    "    for token in tokens:\n",
    "        if token in unigram_dicts[i]:\n",
    "            unigram_dicts[i][token] += 1\n",
    "        else:\n",
    "            unigram_dicts[i][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_dict_all = {}\n",
    "bigram_dict_all = {}\n",
    "\n",
    "for i in range(0,len(filenames)):\n",
    "    file_content = open(DataDir+filenames[i]).read()\n",
    "    tokens = tokenizer.tokenize(file_content)\n",
    "    #populate the unigram\n",
    "    for token in tokens:\n",
    "        if token in unigram_dict_all:\n",
    "            unigram_dict_all[token] += 1\n",
    "        else:\n",
    "            unigram_dict_all[token] = 1\n",
    "    #populate the bigram\n",
    "    for i in range(1,len(tokens)):\n",
    "        bigram = '(' + tokens[i-1] + ' ' + tokens[i] + ')'\n",
    "        if bigram in bigram_dict_all:\n",
    "            bigram_dict_all[bigram] += 1\n",
    "        else:\n",
    "            bigram_dict_all[bigram] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Identify common words between documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 100 texts file.\n",
    "\n",
    "Output: Matrix A where A[i,j] contains the number of unique common tokens (words) between each pair of documents. The diagonal term is just the number of unique terms in each documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.zeros((len(filenames),len(filenames))) #initialize count to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(filenames)):\n",
    "    A[i,i] = len(unigram_dicts[i])\n",
    "    for j in range(i+1, len(filenames)):\n",
    "        for key in unigram_dicts[i].keys():\n",
    "            if key in unigram_dicts[j]:\n",
    "                A[i,j] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Complete the symmetric matrix\n",
    "A = A + A.T - np.diag(A.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write out the output file.\n",
    "np.savetxt(DataDir + 'result1.csv', A, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Identifing words by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 100 texts file.\n",
    "\n",
    "Output: List of 100 lines. The first 50 lines contain the 50 most frequent unigrams. The next 50 lines contain the 50 most frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_unigram = sorted(unigram_dict_all.items(), key=operator.itemgetter(1))\n",
    "sorted_bigram = sorted(bigram_dict_all.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = open(DataDir + 'result2.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in range(len(sorted_unigram)-1,len(sorted_unigram)-51,-1):\n",
    "    f2.write(sorted_unigram[n][0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in range(len(sorted_bigram)-1,len(sorted_bigram)-51,-1):\n",
    "    f2.write(sorted_bigram[n][0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output files look good except that there are common words and numbers. I should get rid off them if I have more time."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
